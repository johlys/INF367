{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124ef1b3",
   "metadata": {},
   "source": [
    "# Spaceship Titanic: Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "Project: [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic/overview)\n",
    "\n",
    "In this notebook, we will:\n",
    "- Build baseline models using the processed dataset\n",
    "- Tune model hyperparameters for better performance\n",
    "- Compare different models\n",
    "- Prepare submission files (if for competition)\n",
    "\n",
    "Our goal here is to train, validate and tune models to to their best for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ced24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "train = pd.read_csv('data/processed_train.csv')\n",
    "test = pd.read_csv('data/processed_test.csv')\n",
    "\n",
    "\n",
    "df_Y = train['Transported']\n",
    "df_X = train.drop(columns=['Transported'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03964a",
   "metadata": {},
   "source": [
    "### Baseline models\n",
    "- Here we simply want to naively run some standard models and get a baseline prediction accuracy.\n",
    "    Particullary we run them several times with different random states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874c6876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model      mean       std\n",
      "0            CatBoost  0.812363  0.011334\n",
      "1          ExtraTrees  0.779758  0.010921\n",
      "2    GradientBoosting  0.800403  0.008233\n",
      "3                 KNN  0.737205  0.012256\n",
      "4            LightGBM  0.806728  0.009658\n",
      "5  LogisticRegression  0.795860  0.010294\n",
      "6                 MLP  0.768660  0.013546\n",
      "7        RandomForest  0.799080  0.012232\n",
      "8                 SVC  0.790684  0.010492\n",
      "9             XGBoost  0.802473  0.009085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_runs = 10  # how many times to run\n",
    "results = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    train_x_run, val_x_run, train_y_run, val_y_run = train_test_split(df_X, df_Y, test_size=0.2, random_state=run, stratify=df_Y)\n",
    "    \n",
    "    # our baseline models\n",
    "    models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=run),\n",
    "    'ExtraTrees': ExtraTreesClassifier(random_state=run),\n",
    "    'LogisticRegression': make_pipeline(StandardScaler(), LogisticRegression(max_iter=5000, random_state=run)),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=run),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=run),\n",
    "    'LightGBM': LGBMClassifier(random_state=run, verbosity = -1),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=run),\n",
    "    'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    'SVC': make_pipeline(StandardScaler(), SVC(probability=True, random_state=run)),\n",
    "    'MLP': make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, random_state=run))\n",
    "    }\n",
    "    \n",
    "    # We train and evaluate\n",
    "    for name, model in models.items():\n",
    "        model.fit(train_x_run, train_y_run)\n",
    "        y_pred = model.predict(val_x_run)\n",
    "        acc = accuracy_score(val_y_run, y_pred)\n",
    "        \n",
    "        results.append({ 'Model': name, 'Accuracy': acc}) # store our results\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# we take the mean and std of our models\n",
    "summary_df = results_df.groupby('Model')['Accuracy'].agg(['mean', 'std']).reset_index()\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc899b6",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "- The results show that boosting models such as **CatBoost**, **GradientBoosting**, **LightGBM**, and **XGBoost** perform the best on this dataset, managing to get validation accuracies around 80.2%-81.2. Among them, **CatBoost** slightly outperforms the others, making it the top candidate without any hyperparameter tuning.\n",
    "\n",
    "\n",
    "- From this roundup we pickout three to parameter tune: **CatBoost**, **LightGBM** and **XGBoost**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd117e5f",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "- To tune parameters we use RandomizedSeachCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8180f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "train_x, val_x, train_y, val_y = train_test_split(df_X,df_Y, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Defining our parameter space\n",
    "catboost_params = {\n",
    "    'depth': [3, 5, 8],\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1],\n",
    "    'iterations': [250, 500, 1000],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'bagging_temperature': [0, 1, 5],\n",
    "    'random_strength': [1, 5, 10]\n",
    "}\n",
    "\n",
    "lightgbm_params = {\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [250, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'reg_alpha': [0, 0.1, 1],  # l1 regularization\n",
    "    'reg_lambda': [1, 3, 5]    # l2 regularization\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50,50), (100,50), (100,100)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e1be3",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cecc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost parameters: {'random_strength': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 250, 'depth': 5, 'bagging_temperature': 0}\n",
      "Best CatBoost CV Accuracy: 0.8142353121644424\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostClassifier(verbose=0, random_state=SEED)\n",
    "\n",
    "cat_random = RandomizedSearchCV(\n",
    "    cat_model,\n",
    "    param_distributions=catboost_params,\n",
    "    n_iter=200,  # number of random combos to try\n",
    "    cv=3,       # 3-fold CV\n",
    "    scoring='accuracy',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cat_random.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best CatBoost parameters:\", cat_random.best_params_)\n",
    "print(\"Best CatBoost CV Accuracy:\", cat_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4385b9b",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06cfccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/anaconda3/envs/INF265/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM parameters: {'subsample': 0.8, 'num_leaves': 31, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "Best LightGBM CV Accuracy: 0.8091731860714834\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier(random_state=SEED, verbosity = -1)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=lightgbm_params,\n",
    "    n_iter=200,     # only 10 random trials\n",
    "    cv=3,          # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best LightGBM parameters:\", random_search.best_params_)\n",
    "print(\"Best LightGBM CV Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb0309",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f572d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/anaconda3/envs/INF265/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/johannes/anaconda3/envs/INF265/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "Best XGBoost CV Accuracy: 0.8110139591961958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = XGBClassifier(random_state=SEED, eval_metric='logloss')\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=200,     # only 10 random trials\n",
    "    cv=3,          # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best XGBoost parameters:\", random_search.best_params_)\n",
    "print(\"Best XGBoost CV Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba51f44",
   "metadata": {},
   "source": [
    "### Result\n",
    "- Again **CatBoost** perform the best out of the models with an accuracy ~81.42%\n",
    "- We train a new **CatBoost** model with the chosen parameters and the full training dataset.\n",
    "\n",
    "Parameters:\n",
    "* 'random_strength': 1\n",
    "* 'learning_rate': 0.05\n",
    "* 'l2_leaf_reg': 5\n",
    "* 'iterations': 250\n",
    "* 'depth': 5\n",
    "* 'bagging_temperature': 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0956e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruuning our final standard model and getting our predictions\n",
    "final_model = CatBoostClassifier(verbose=0, random_state=SEED,\n",
    "                                 random_strength=1,learning_rate=0.05,\n",
    "                                 l2_leaf_reg=5, iterations=250,\n",
    "                                 depth=5, bagging_temperature=0)\n",
    "\n",
    "\n",
    "final_model.fit(df_X, df_Y) # we use all our training data for our final model.\n",
    "test_pred = final_model.predict(test)\n",
    "\n",
    "test_ids = pd.read_csv('data/test.csv')['PassengerId'] # get passengerId back, (removed during encoding)\n",
    "\n",
    "submission_catboost = pd.DataFrame({ # format the submission\n",
    "    'PassengerId': test_ids,\n",
    "    'Transported': test_pred\n",
    "})\n",
    "submission_catboost.to_csv('submissions/submission_cat.csv', index=False) # store the submission as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb86596",
   "metadata": {},
   "source": [
    "### Result\n",
    "- This CatBoost model got a score of 0.80406 on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f42738-c187-437f-8c61-f84bde12ed90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
