{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124ef1b3",
   "metadata": {},
   "source": [
    "# Spaceship Titanic: Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "Project: [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic/overview)\n",
    "\n",
    "In this notebook, we will:\n",
    "- Build baseline models using the processed dataset\n",
    "- Tune model hyperparameters for better performance\n",
    "- Compare different models\n",
    "- Prepare submission files for Kaggle\n",
    "\n",
    "Our goal here is to train, validate and tune models to to their best for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ced24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "SEED = 42 # define our random seed\n",
    "\n",
    "train = pd.read_csv('data/processed_train.csv')\n",
    "test = pd.read_csv('data/processed_test.csv')\n",
    "\n",
    "\n",
    "df_Y = train['Transported']\n",
    "df_X = train.drop(columns=['Transported'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03964a",
   "metadata": {},
   "source": [
    "### Baseline models\n",
    "- Here we simply want to naively run some standard models and get a baseline prediction accuracy to assess which models we should devote time to tune.\n",
    "Particulary we run them several times with different random states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c6876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model      mean       std\n",
      "0            CatBoost  0.812363  0.011334\n",
      "1          ExtraTrees  0.779758  0.010921\n",
      "2    GradientBoosting  0.800403  0.008233\n",
      "3                 KNN  0.737205  0.012256\n",
      "4            LightGBM  0.806728  0.009658\n",
      "5  LogisticRegression  0.795860  0.010294\n",
      "6                 MLP  0.768660  0.013546\n",
      "7        RandomForest  0.799080  0.012232\n",
      "8                 SVC  0.790684  0.010492\n",
      "9             XGBoost  0.802473  0.009085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_runs = 10  # number of different random seeds/runs\n",
    "results = [] # where we will collect the results\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # split into train and validation for this run, using stratification to preserve class balance\n",
    "    train_x_run, val_x_run, train_y_run, val_y_run = train_test_split(df_X, df_Y, test_size=0.2, random_state=run, stratify=df_Y)\n",
    "    \n",
    "    # define a set of baseline models to train\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(random_state=run), # basic random forest\n",
    "        'ExtraTrees': ExtraTreesClassifier(random_state=run), # extra trees ensemble\n",
    "        'LogisticRegression': make_pipeline(StandardScaler(), LogisticRegression(max_iter=5000, random_state=run)), # logistic regression with scaling # we define max iter since default causes small error\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=run), # gradient boosting trees\n",
    "        'XGBoost': XGBClassifier(eval_metric='logloss', random_state=run), # boosted trees via XGBoost\n",
    "        'LightGBM': LGBMClassifier(random_state=run, verbosity=-1), # boosted trees via LightGBM\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=run), # CatBoost for categorical-friendly boosting\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),  # k-nearest neighbors with scaling (important because KNN is distance-based)\n",
    "        'SVC': make_pipeline(StandardScaler(), SVC(probability=True, random_state=run)), # support vector classifier with scaling (important because SVMs are sensitive to feature scales)\n",
    "        'MLP': make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, random_state=run)) # basic multi-layer perceptron with scaling (helps neural nets converge better)\n",
    "    }\n",
    "    \n",
    "    # train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        model.fit(train_x_run, train_y_run) # train the model\n",
    "        y_pred = model.predict(val_x_run) # predict on the validation split\n",
    "        acc = accuracy_score(val_y_run, y_pred) # calculate accuracy\n",
    "        \n",
    "        results.append({'Model': name, 'Accuracy': acc}) # save the result\n",
    "\n",
    "# create a DataFrame from all runs\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# compute the mean and standard deviation accuracy for each model across all runs\n",
    "summary_df = results_df.groupby('Model')['Accuracy'].agg(['mean', 'std']).reset_index()\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc899b6",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "- The results show that boosting models such as **CatBoost**, **GradientBoosting**, **LightGBM**, and **XGBoost** achieve the highest validation accuracies on this dataset, consistently reaching around 80.2% to 81.2%. Among these, **CatBoost** slightly outperforms the others, making it the strongest candidate without any hyperparameter tuning.\n",
    "\n",
    "- Based on these findings, we selected **CatBoost**, **LightGBM**, and **XGBoost** for further hyperparameter optimization in the next stage of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd117e5f",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "- To tune parameters we use RandomizedSeachCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and validation sets\n",
    "train_x, val_x, train_y, val_y = train_test_split(df_X, df_Y, test_size=0.25, shuffle=True, random_state=SEED) \n",
    "# 25% validation split to leave enough data for training while still having a robust validation set\n",
    "# shuffle=True ensures random mixing\n",
    "# random_state=SEED for reproducibility across runs\n",
    "\n",
    "\n",
    "# Defining parameter grids for hyperparameter tuning of our main baseline models\n",
    "\n",
    "# --- CatBoost hyperparameters ---\n",
    "catboost_params = {\n",
    "    'depth': [3, 5, 8], # controls tree depth; deeper trees can capture more complex patterns but may overfit\n",
    "    'learning_rate': [0.005, 0.01, 0.05, 0.1], # lower learning rates usually mean slower but safer convergence\n",
    "    'iterations': [250, 500, 1000], # how many boosting rounds; more rounds can fit better but risk overfitting if too many\n",
    "    'l2_leaf_reg': [1, 3, 5, 7], # L2 regularization to penalize large weights, helps avoid overfitting\n",
    "    'bagging_temperature': [0, 1, 5], # controls randomness in bagging; higher values add more randomness, can help generalization\n",
    "    'random_strength': [1, 5, 10] # randomness for feature splits; again helps regularize the model\n",
    "}\n",
    "\n",
    "# --- LightGBM hyperparameters ---\n",
    "lightgbm_params = {\n",
    "    'num_leaves': [31, 63, 127], # number of leaves controls model complexity; larger values = more complex model\n",
    "    'learning_rate': [0.01, 0.05, 0.1], # learning rate tradeoff: slower rates may give better results if you can afford more training\n",
    "    'n_estimators': [500, 1000], # total number of boosting rounds; tied to learning rate\n",
    "    'max_depth': [4, 6, 8], # limit maximum depth to avoid very large trees that overfit\n",
    "    'subsample': [0.8, 1.0], # randomly sample part of data for each tree (bagging); helps prevent overfitting\n",
    "    'colsample_bytree': [0.8, 1.0] # randomly sample part of features per tree; another regularization method\n",
    "}\n",
    "\n",
    "# --- XGBoost hyperparameters ---\n",
    "xgb_params = {\n",
    "    'n_estimators': [250, 500, 1000], # number of trees (same logic: more trees = better fit, more overfitting risk)\n",
    "    'learning_rate': [0.01, 0.05, 0.1], # small learning rates are safer but need more trees\n",
    "    'max_depth': [3, 5, 7], # smaller depths generalize better; deeper trees can overfit\n",
    "    'subsample': [0.8, 1.0], # bagging fraction of data; reduces overfitting\n",
    "    'colsample_bytree': [0.8, 1.0], # feature bagging; forces model to not rely on all features all the time\n",
    "    'gamma': [0, 0.1, 0.5], # minimum loss reduction required to make a split; adds pruning effect\n",
    "    'reg_alpha': [0, 0.1, 1], # L1 regularization term (sparsity), useful for feature selection\n",
    "    'reg_lambda': [1, 3, 5]  # L2 regularization term (weight shrinkage), helps with overfitting\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e1be3",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cecc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost parameters: {'random_strength': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 250, 'depth': 5, 'bagging_temperature': 0}\n",
      "Best CatBoost CV Accuracy: 0.8142353121644424\n"
     ]
    }
   ],
   "source": [
    "# Initialize the base CatBoost model\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=SEED) \n",
    "# verbose=0 to keep output clean\n",
    "# setting random_state ensures reproducibility\n",
    "\n",
    "# Setting up the Randomized Search for hyperparameter tuning\n",
    "cat_random = RandomizedSearchCV(\n",
    "    cat_model,\n",
    "    param_distributions=catboost_params, # search space defined earlier\n",
    "    n_iter=200,  # number of random combinations to try — high enough for good coverage without taking forever\n",
    "    cv=3,        # 3-fold cross-validation: balances between speed and a reliable estimate of model performance\n",
    "    scoring='accuracy', # optimize based on accuracy (since that's the competition metric)\n",
    "    random_state=SEED, # reproducibility: makes search results consistent if rerun\n",
    "    n_jobs=-1 # use all available CPU cores for faster search\n",
    ")\n",
    "\n",
    "# Run the randomized hyperparameter search\n",
    "cat_random.fit(train_x, train_y)\n",
    "\n",
    "# Print the best parameters and cross-validation accuracy score\n",
    "print(\"Best CatBoost parameters:\", cat_random.best_params_)\n",
    "print(\"Best CatBoost CV Accuracy:\", cat_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4385b9b",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/anaconda3/envs/INF265/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM parameters: {'subsample': 0.8, 'num_leaves': 31, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n",
      "Best LightGBM CV Accuracy: 0.8091731860714834\n"
     ]
    }
   ],
   "source": [
    "# Initialize the base LightGBM model\n",
    "model = LGBMClassifier(random_state=SEED, verbosity=-1) \n",
    "# random_state for reproducibility\n",
    "# verbosity=-1 to suppress LightGBM output and keep logs clean\n",
    "\n",
    "# Setting up the Randomized Search for LightGBM\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=lightgbm_params, # search space defined earlier\n",
    "    n_iter=200,  # number of random combinations to try — enough to explore the space without taking too long\n",
    "    cv=3,        # 3-fold cross-validation: common choice that balances speed and reliability\n",
    "    scoring='accuracy', # optimizing for accuracy (competition metric)\n",
    "    random_state=SEED, # reproducibility: same random trials every time\n",
    "    n_jobs=-1 # use all CPU cores for faster search\n",
    ")\n",
    "\n",
    "# Run the randomized hyperparameter search\n",
    "random_search.fit(train_x, train_y)\n",
    "\n",
    "# Print the best parameters and cross-validation score\n",
    "print(\"Best LightGBM parameters:\", random_search.best_params_)\n",
    "print(\"Best LightGBM CV Accuracy:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb0309",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f572d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/anaconda3/envs/INF265/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/johannes/anaconda3/envs/INF265/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "Best XGBoost CV Accuracy: 0.8110139591961958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the base XGBoost model\n",
    "model = XGBClassifier(random_state=SEED, eval_metric='logloss') \n",
    "# random_state for reproducibility\n",
    "# eval_metric='logloss' is set manually because XGBoost needs an explicit evaluation metric to behave correctly for classification\n",
    "\n",
    "# Setting up the Randomized Search for XGBoost\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=xgb_params, # search space defined earlier\n",
    "    n_iter=200,  # number of random parameter combinations to try — large enough for good coverage, faster than exhaustive grid search\n",
    "    cv=3,        # 3-fold cross-validation: balances between training time and reliable evaluation\n",
    "    scoring='accuracy', # targeting accuracy as the optimization metric (fits Kaggle competition goal)\n",
    "    random_state=SEED, # ensures consistent trial results across runs\n",
    "    n_jobs=-1 # use all CPU cores to parallelize the search and speed up\n",
    ")\n",
    "\n",
    "# Run the randomized hyperparameter search\n",
    "random_search.fit(train_x, train_y)\n",
    "\n",
    "# Print the best parameters and cross-validation score\n",
    "print(\"Best XGBoost parameters:\", random_search.best_params_)\n",
    "print(\"Best XGBoost CV Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba51f44",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Once again, **CatBoost** achieved the best performance among the models, reaching a cross-validation accuracy of approximately **81.42%**.\n",
    "- Based on the selected hyperparameters, we retrained a new **CatBoost** model on the full training dataset to maximize its final performance.\n",
    "\n",
    "The best-found parameters were:\n",
    "- `random_strength`: 1\n",
    "- `learning_rate`: 0.05\n",
    "- `l2_leaf_reg`: 5\n",
    "- `iterations`: 250\n",
    "- `depth`: 5\n",
    "- `bagging_temperature`: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c3c95",
   "metadata": {},
   "source": [
    "#### Training the final model and making the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running our final CatBoost model and generating predictions\n",
    "\n",
    "# Initialize the final CatBoost model with the best-found hyperparameters\n",
    "final_model = CatBoostClassifier(\n",
    "    verbose=0, \n",
    "    random_state=SEED,\n",
    "    random_strength=1,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=5,\n",
    "    iterations=250,\n",
    "    depth=5,\n",
    "    bagging_temperature=0\n",
    ")\n",
    "\n",
    "# Train the model on the full training dataset (no validation split, we use all training data)\n",
    "final_model.fit(df_X, df_Y)\n",
    "\n",
    "# Predict Transported status on the test set\n",
    "test_pred = final_model.predict(test)\n",
    "\n",
    "# Load the PassengerId column back in (it was dropped during preprocessing)\n",
    "test_ids = pd.read_csv('data/test.csv')['PassengerId']\n",
    "\n",
    "# Format the submission file according to Kaggle rules\n",
    "submission_catboost = pd.DataFrame({\n",
    "    'PassengerId': test_ids,\n",
    "    'Transported': test_pred\n",
    "})\n",
    "\n",
    "# Save the submission file as a CSV\n",
    "submission_catboost.to_csv('submissions/submission_cat.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb86596",
   "metadata": {},
   "source": [
    "### Result\n",
    "- The final **CatBoost** model achieved a Kaggle leaderboard score of **0.80406**, confirming its strong performance after hyperparameter tuning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
